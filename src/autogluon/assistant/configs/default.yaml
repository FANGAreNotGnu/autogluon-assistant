# Tutorial Prompt Generator Configuration

per_execution_timeout: 7200

# MTCS
exploration_constant: 1.414
max_debug_depth: 5
max_evolve_attempts: 10
max_debug_attempts: 5
metric_improvement_threshold: 5

# Data Perception
max_file_group_size_to_show: 5
num_example_files_to_show: 1

max_chars_per_file: 768
num_tutorial_retrievals: 30
max_num_tutorials: 5
max_user_input_length: 2048
max_tutorial_length: 32768
configure_env: false
condense_tutorials: True
use_tutorial_summary: True
continuous_improvement: False
optimize_system_resources: False
cleanup_unused_env: True
enable_meta_prompting: False

# Default LLM Configuration
# For each agent (coder, etc.) you can use a different one
llm: &default_llm
  # Note: bedrock is only supported in limited AWS regions
  #       and requires AWS credentials
  provider: bedrock
  model: "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
  #provider: openai
  #model: gpt-4o-2024-08-06
  #provider: anthropic
  # model: claude-3-7-sonnet-20250219
  max_tokens: 65535
  proxy_url: null
  temperature: 0.1
  top_p: 0.9
  verbose: True
  multi_turn: False
  template: null
  add_coding_format_instruction: false
  apply_meta_prompting: False

python_coder:
  <<: *default_llm  # Merge llm_config
  multi_turn: True
  apply_meta_prompting: True

bash_coder:
  <<: *default_llm  # Merge llm_config
  multi_turn: True

executer:
  <<: *default_llm  # Merge llm_config
  
meta_prompting:
  <<: *default_llm  # Merge llm_config
  multi_turn: False

reader:
  <<: *default_llm  # Merge llm_config
  details: False

error_analyzer:
  <<: *default_llm  # Merge llm_config

retriever:
  <<: *default_llm  # Merge llm_config

reranker:
  <<: *default_llm  # Merge llm_config
  temperature: 0.
  top_p: 1.

description_file_retriever:
  <<: *default_llm  # Merge llm_config
  temperature: 0.
  top_p: 1.

task_descriptor:
  <<: *default_llm  # Merge llm_config
  max_description_files_length_to_show: 1024
  max_description_files_length_for_summarization: 16384
  apply_meta_prompting: True

tool_selector:
  <<: *default_llm  # Merge llm_config
  temperature: 0.
  top_p: 1.
